{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Notebook 09: Model Training - Support Vector Machine (SVM)\n",
    "\n",
    "**Objective:** Train and evaluate SVM classifier for crop recommendation\n",
    "\n",
    "**Contents:**\n",
    "1. Import Libraries and Load Data\n",
    "2. SVM Algorithm Overview\n",
    "3. Train Multiple SVM Models (Linear, RBF, Poly kernels)\n",
    "4. Hyperparameter Tuning with GridSearchCV\n",
    "5. Model Evaluation and Performance Metrics\n",
    "6. Confusion Matrix Visualization\n",
    "7. Support Vectors Analysis\n",
    "8. Model Comparison and Selection\n",
    "9. Save Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, classification_report, confusion_matrix,\n",
    "                             roc_curve, auc, roc_auc_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK 09: MODEL TRAINING - SUPPORT VECTOR MACHINE (SVM)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "# 2. Load Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-train-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing sets (Standard scaled)\n",
    "X_train = np.load(\"../data/processed/ml_ready/X_train_scaled.npy\")\n",
    "X_test = np.load(\"../data/processed/ml_ready/X_test_scaled.npy\")\n",
    "y_train = np.load(\"../data/processed/ml_ready/y_train.npy\")\n",
    "y_test = np.load(\"../data/processed/ml_ready/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-feature-names",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature names\n",
    "with open(\"../data/processed/ml_ready/feature_names.pkl\", \"rb\") as f:\n",
    "    feature_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-label-encoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label encoder\n",
    "with open(\"../models/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä DATASET SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of features: {X_train.shape[1]:,}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train)):,}\")\n",
    "print(f\"\\nClass labels: {label_encoder.classes_.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svm-overview-header",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machine - Algorithm Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = \"\"\"\n",
    "üìö SUPPORT VECTOR MACHINE (SVM):\n",
    "\n",
    "üéØ TYPE: Maximum Margin Classification Algorithm\n",
    "\n",
    "üìê MATHEMATICAL CONCEPT:\n",
    "   ‚Ä¢ Finds the optimal hyperplane that maximizes the margin between classes\n",
    "   ‚Ä¢ Decision boundary: w¬∑x + b = 0\n",
    "   ‚Ä¢ Support vectors are the closest points to the decision boundary\n",
    "   \n",
    "   Optimization objective:\n",
    "   minimize: (1/2)||w||¬≤ + C √ó Œ£Œæ·µ¢\n",
    "   subject to: y·µ¢(w¬∑x·µ¢ + b) ‚â• 1 - Œæ·µ¢\n",
    "   \n",
    "   where:\n",
    "   ‚Ä¢ w = Weight vector (hyperplane normal)\n",
    "   ‚Ä¢ b = Bias term\n",
    "   ‚Ä¢ C = Regularization parameter\n",
    "   ‚Ä¢ Œæ·µ¢ = Slack variables for soft margin\n",
    "\n",
    "üîÑ KERNEL FUNCTIONS:\n",
    "   ‚Ä¢ Linear: K(x,y) = x¬∑y\n",
    "   ‚Ä¢ RBF (Gaussian): K(x,y) = exp(-Œ≥||x-y||¬≤)\n",
    "   ‚Ä¢ Polynomial: K(x,y) = (Œ≥x¬∑y + r)^d\n",
    "   ‚Ä¢ Sigmoid: K(x,y) = tanh(Œ≥x¬∑y + r)\n",
    "\n",
    "üîÑ FOR MULTI-CLASS (22 crops):\n",
    "   ‚Ä¢ Uses One-vs-One (OvO) or One-vs-Rest (OvR) strategy\n",
    "   ‚Ä¢ OvO: Creates n(n-1)/2 = 231 binary classifiers\n",
    "   ‚Ä¢ OvR: Creates n = 22 binary classifiers\n",
    "\n",
    "‚úÖ ADVANTAGES:\n",
    "   ‚Ä¢ Effective in high-dimensional spaces\n",
    "   ‚Ä¢ Works well with non-linear boundaries (kernel trick)\n",
    "   ‚Ä¢ Robust to outliers (margin maximization)\n",
    "   ‚Ä¢ Memory efficient (only stores support vectors)\n",
    "   ‚Ä¢ Excellent generalization capability\n",
    "\n",
    "‚ùå LIMITATIONS:\n",
    "   ‚Ä¢ Slow training on large datasets (O(n¬≤) to O(n¬≥))\n",
    "   ‚Ä¢ Sensitive to feature scaling (requires normalization)\n",
    "   ‚Ä¢ Choice of kernel and hyperparameters is crucial\n",
    "   ‚Ä¢ Doesn't provide probability estimates directly\n",
    "\n",
    "üéØ BEST FOR:\n",
    "   ‚Ä¢ Medium-sized datasets\n",
    "   ‚Ä¢ Binary and multi-class classification\n",
    "   ‚Ä¢ High-dimensional feature spaces\n",
    "   ‚Ä¢ When margin of separation is important\n",
    "\"\"\"\n",
    "\n",
    "print(overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-base-header",
   "metadata": {},
   "source": [
    "# 4. Train Base SVM Models with Different Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-kernels",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SVM MODELS WITH DIFFERENT KERNELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define kernel configurations\n",
    "kernel_configs = {\n",
    "    'Linear': {'kernel': 'linear', 'C': 1.0},\n",
    "    'RBF': {'kernel': 'rbf', 'C': 1.0, 'gamma': 'scale'},\n",
    "    'Polynomial': {'kernel': 'poly', 'C': 1.0, 'degree': 3, 'gamma': 'scale'}\n",
    "}\n",
    "\n",
    "# Store results\n",
    "svm_models = {}\n",
    "kernel_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-kernels",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_name, params in kernel_configs.items():\n",
    "    print(f\"\\nüîÑ Training SVM with {kernel_name} kernel...\")\n",
    "    print(f\"   Parameters: {params}\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    start_time = time.time()\n",
    "    svm_model = SVC(**params, random_state=42, probability=True)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    start_time = time.time()\n",
    "    y_train_pred = svm_model.predict(X_train)\n",
    "    y_test_pred = svm_model.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Store model and results\n",
    "    svm_models[kernel_name] = svm_model\n",
    "    kernel_results.append({\n",
    "        'Kernel': kernel_name,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1-Score': test_f1,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Prediction Time (s)': prediction_time,\n",
    "        'Support Vectors': svm_model.n_support_.sum(),\n",
    "        'Overfitting Gap': train_accuracy - test_accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"   ‚úÖ Completed in {training_time:.4f} seconds\")\n",
    "    print(f\"   ‚Ä¢ Train Accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Support Vectors: {svm_model.n_support_.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kernel-comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display kernel comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KERNEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(kernel_results)\n",
    "print(\"\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gridsearch-header",
   "metadata": {},
   "source": [
    "# 5. Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gridsearch-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRIDSEARCHCV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define parameter grid for RBF kernel (typically best performer)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Parameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   ‚Ä¢ {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\n   Total combinations: {total_combinations}\")\n",
    "print(f\"   With 5-fold CV: {total_combinations * 5} fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-gridsearch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Running GridSearchCV (this may take a few minutes)...\")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "svm_grid = GridSearchCV(\n",
    "    estimator=SVC(random_state=42, probability=True),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "start_time = time.time()\n",
    "svm_grid.fit(X_train, y_train)\n",
    "grid_search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ GridSearchCV completed in {grid_search_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüèÜ Best Parameters:\")\n",
    "for param, value in svm_grid.best_params_.items():\n",
    "    print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best Cross-Validation Score: {svm_grid.best_score_*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 hyperparameter combinations\n",
    "print(\"\\nüìã Top 10 Hyperparameter Combinations:\")\n",
    "\n",
    "cv_results_df = pd.DataFrame(svm_grid.cv_results_)\n",
    "cv_results_df = cv_results_df.sort_values('rank_test_score')\n",
    "\n",
    "top_10 = cv_results_df[['params', 'mean_test_score', 'std_test_score', 'mean_train_score', 'rank_test_score']].head(10)\n",
    "top_10['mean_test_score'] = top_10['mean_test_score'] * 100\n",
    "top_10['mean_train_score'] = top_10['mean_train_score'] * 100\n",
    "top_10['std_test_score'] = top_10['std_test_score'] * 100\n",
    "\n",
    "print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-model-header",
   "metadata": {},
   "source": [
    "# 6. Train Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-best-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL SVM MODEL WITH BEST PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get best model\n",
    "best_svm = svm_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nüîÑ Making predictions...\")\n",
    "start_time = time.time()\n",
    "y_train_pred = best_svm.predict(X_train)\n",
    "y_train_pred_proba = best_svm.predict_proba(X_train)\n",
    "train_pred_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "y_test_pred_proba = best_svm.predict_proba(X_test)\n",
    "test_pred_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Train prediction time: {train_pred_time:.4f} seconds\")\n",
    "print(f\"   Test prediction time: {test_pred_time:.4f} seconds\")\n",
    "print(f\"   Avg prediction time per sample: {test_pred_time/len(y_test)*1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"\\nüìã Sample Predictions (First 10 test samples):\")\n",
    "sample_df = pd.DataFrame({\n",
    "    \"True Label\": [label_encoder.classes_[i] for i in y_test[:10]],\n",
    "    \"Predicted\": [label_encoder.classes_[i] for i in y_test_pred[:10]],\n",
    "    \"Confidence\": [np.max(y_test_pred_proba[i]) * 100 for i in range(10)],\n",
    "    \"Match\": [\"‚úÖ\" if y_test[i] == y_test_pred[i] else \"‚ùå\" for i in range(10)]\n",
    "})\n",
    "\n",
    "print(sample_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "# 7. Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training set metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "train_recall = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Testing set metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-train-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä TRAINING SET PERFORMANCE:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Accuracy:  {train_accuracy * 100:.2f}%\")\n",
    "print(f\"   Precision: {train_precision * 100:.2f}%\")\n",
    "print(f\"   Recall:    {train_recall * 100:.2f}%\")\n",
    "print(f\"   F1-Score:  {train_f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-test-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä TESTING SET PERFORMANCE:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Accuracy:  {test_accuracy * 100:.2f}%\")\n",
    "print(f\"   Precision: {test_precision * 100:.2f}%\")\n",
    "print(f\"   Recall:    {test_recall * 100:.2f}%\")\n",
    "print(f\"   F1-Score:  {test_f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfitting-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting analysis\n",
    "print(\"\\nüîç OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "accuracy_diff = train_accuracy - test_accuracy\n",
    "print(f\"   Train-Test Accuracy Gap: {accuracy_diff * 100:.2f}%\")\n",
    "\n",
    "if accuracy_diff < 0.02:\n",
    "    status = \"‚úÖ Excellent - No overfitting detected\"\n",
    "elif accuracy_diff < 0.05:\n",
    "    status = \"üü° Good - Mild overfitting\"\n",
    "elif accuracy_diff < 0.10:\n",
    "    status = \"üü† Moderate - Noticeable overfitting\"\n",
    "else:\n",
    "    status = \"üî¥ Severe - Significant overfitting\"\n",
    "\n",
    "print(f\"   Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-report-header",
   "metadata": {},
   "source": [
    "# 8. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (Testing Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report = classification_report(y_test, y_test_pred, target_names=label_encoder.classes_, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classification report to CSV\n",
    "report_dict = classification_report(y_test, y_test_pred, target_names=label_encoder.classes_, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df = report_df.round(4)\n",
    "report_df.to_csv('../data/results/svm_classification_report.csv')\n",
    "print(\"\\n‚úÖ Classification report saved to: ../data/results/svm_classification_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix-header",
   "metadata": {},
   "source": [
    "# 9. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFUSION MATRIX VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_,\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('SVM Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].tick_params(axis='both', labelsize=8)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('SVM Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].tick_params(axis='both', labelsize=8)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/visualizations/40_svm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Confusion matrix saved to: ../data/visualizations/40_svm_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "support-vectors-header",
   "metadata": {},
   "source": [
    "# 10. Support Vectors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "support-vectors-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUPPORT VECTORS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Total Support Vectors: {best_svm.n_support_.sum()}\")\n",
    "print(f\"   Percentage of training data: {best_svm.n_support_.sum()/len(y_train)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüìã Support Vectors per Class:\")\n",
    "sv_per_class = pd.DataFrame({\n",
    "    'Class': label_encoder.classes_,\n",
    "    'Support Vectors': best_svm.n_support_,\n",
    "    'Percentage': (best_svm.n_support_ / best_svm.n_support_.sum() * 100).round(2)\n",
    "})\n",
    "print(sv_per_class.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-support-vectors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize support vectors per class\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors = plt.cm.husl(np.linspace(0, 1, len(label_encoder.classes_)))\n",
    "bars = ax.bar(label_encoder.classes_, best_svm.n_support_, color=colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Crop Class', fontsize=12)\n",
    "ax.set_ylabel('Number of Support Vectors', fontsize=12)\n",
    "ax.set_title('Support Vectors Distribution by Crop Class', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, best_svm.n_support_):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            str(value), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/visualizations/41_svm_support_vectors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Support vectors plot saved to: ../data/visualizations/41_svm_support_vectors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "per-class-header",
   "metadata": {},
   "source": [
    "# 11. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-class-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create per-class performance visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Precision per class\n",
    "precision_per_class = precision_score(y_test, y_test_pred, average=None, zero_division=0)\n",
    "axes[0].barh(label_encoder.classes_, precision_per_class, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision by Crop', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(x=np.mean(precision_per_class), color='red', linestyle='--', label=f'Mean: {np.mean(precision_per_class):.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Recall per class\n",
    "recall_per_class = recall_score(y_test, y_test_pred, average=None, zero_division=0)\n",
    "axes[1].barh(label_encoder.classes_, recall_per_class, color='forestgreen', edgecolor='black')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_title('Recall by Crop', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=np.mean(recall_per_class), color='red', linestyle='--', label=f'Mean: {np.mean(recall_per_class):.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# F1-Score per class\n",
    "f1_per_class = f1_score(y_test, y_test_pred, average=None, zero_division=0)\n",
    "axes[2].barh(label_encoder.classes_, f1_per_class, color='coral', edgecolor='black')\n",
    "axes[2].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[2].set_title('F1-Score by Crop', fontsize=14, fontweight='bold')\n",
    "axes[2].axvline(x=np.mean(f1_per_class), color='red', linestyle='--', label=f'Mean: {np.mean(f1_per_class):.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/visualizations/42_svm_per_class_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Per-class performance plot saved to: ../data/visualizations/42_svm_per_class_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify-difficult-classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best and difficult classes\n",
    "print(\"\\nüèÜ BEST PERFORMING CLASSES (F1-Score):\")\n",
    "class_performance = pd.DataFrame({\n",
    "    'Class': label_encoder.classes_,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 classes:\")\n",
    "print(class_performance.head().to_string(index=False))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è CLASSES NEEDING IMPROVEMENT:\")\n",
    "print(class_performance.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidence-header",
   "metadata": {},
   "source": [
    "# 12. Prediction Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidence-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate confidence scores\n",
    "confidence_scores = np.max(y_test_pred_proba, axis=1) * 100\n",
    "\n",
    "# Confidence for correct vs incorrect predictions\n",
    "correct_mask = y_test == y_test_pred\n",
    "correct_confidence = confidence_scores[correct_mask]\n",
    "incorrect_confidence = confidence_scores[~correct_mask]\n",
    "\n",
    "print(f\"\\nüìä Confidence Statistics:\")\n",
    "print(f\"   Overall mean confidence: {np.mean(confidence_scores):.2f}%\")\n",
    "print(f\"   Correct predictions mean: {np.mean(correct_confidence):.2f}%\")\n",
    "if len(incorrect_confidence) > 0:\n",
    "    print(f\"   Incorrect predictions mean: {np.mean(incorrect_confidence):.2f}%\")\n",
    "print(f\"   Min confidence: {np.min(confidence_scores):.2f}%\")\n",
    "print(f\"   Max confidence: {np.max(confidence_scores):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(confidence_scores, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=np.mean(confidence_scores), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {np.mean(confidence_scores):.1f}%')\n",
    "axes[0].set_xlabel('Confidence Score (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('SVM Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot comparing correct vs incorrect\n",
    "if len(incorrect_confidence) > 0:\n",
    "    data_to_plot = [correct_confidence, incorrect_confidence]\n",
    "    bp = axes[1].boxplot(data_to_plot, labels=['Correct', 'Incorrect'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightgreen')\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "else:\n",
    "    bp = axes[1].boxplot([correct_confidence], labels=['Correct'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightgreen')\n",
    "    \n",
    "axes[1].set_ylabel('Confidence Score (%)', fontsize=12)\n",
    "axes[1].set_title('Confidence: Correct vs Incorrect Predictions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/visualizations/43_svm_confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Confidence distribution saved to: ../data/visualizations/43_svm_confidence_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-summary-header",
   "metadata": {},
   "source": [
    "# 13. Model Summary and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SVM MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    'Model': 'Support Vector Machine',\n",
    "    'Kernel': best_svm.kernel,\n",
    "    'C (Regularization)': best_svm.C,\n",
    "    'Gamma': best_svm.gamma if hasattr(best_svm, 'gamma') else 'N/A',\n",
    "    'Train Accuracy': f'{train_accuracy*100:.2f}%',\n",
    "    'Test Accuracy': f'{test_accuracy*100:.2f}%',\n",
    "    'Test Precision': f'{test_precision*100:.2f}%',\n",
    "    'Test Recall': f'{test_recall*100:.2f}%',\n",
    "    'Test F1-Score': f'{test_f1*100:.2f}%',\n",
    "    'Overfitting Gap': f'{accuracy_diff*100:.2f}%',\n",
    "    'Total Support Vectors': best_svm.n_support_.sum(),\n",
    "    'Training Time (GridSearch)': f'{grid_search_time:.2f}s'\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Model Configuration:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-summary-csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Metric': 'Test Accuracy',\n",
    "    'Value': f'{test_accuracy*100:.2f}%'\n",
    "}, {\n",
    "    'Metric': 'Test Precision',\n",
    "    'Value': f'{test_precision*100:.2f}%'\n",
    "}, {\n",
    "    'Metric': 'Test Recall',\n",
    "    'Value': f'{test_recall*100:.2f}%'\n",
    "}, {\n",
    "    'Metric': 'Test F1-Score',\n",
    "    'Value': f'{test_f1*100:.2f}%'\n",
    "}, {\n",
    "    'Metric': 'Train Accuracy',\n",
    "    'Value': f'{train_accuracy*100:.2f}%'\n",
    "}, {\n",
    "    'Metric': 'Overfitting Gap',\n",
    "    'Value': f'{accuracy_diff*100:.2f}%'\n",
    "}, {\n",
    "    'Metric': 'Kernel',\n",
    "    'Value': best_svm.kernel\n",
    "}, {\n",
    "    'Metric': 'C Parameter',\n",
    "    'Value': str(best_svm.C)\n",
    "}, {\n",
    "    'Metric': 'Support Vectors',\n",
    "    'Value': str(best_svm.n_support_.sum())\n",
    "}])\n",
    "\n",
    "summary_df.to_csv('../data/results/svm_summary.csv', index=False)\n",
    "print(\"\\n‚úÖ Summary saved to: ../data/results/svm_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"\\nüíæ SAVING SVM MODEL...\")\n",
    "\n",
    "with open('../models/svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_svm, f)\n",
    "\n",
    "print(\"‚úÖ Model saved to: ../models/svm_model.pkl\")\n",
    "\n",
    "# Verify saved model\n",
    "import os\n",
    "model_size = os.path.getsize('../models/svm_model.pkl') / 1024\n",
    "print(f\"   Model size: {model_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for comparison\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label_Encoded': y_test,\n",
    "    'Predicted_Label_Encoded': y_test_pred,\n",
    "    'True_Label': [label_encoder.classes_[i] for i in y_test],\n",
    "    'Predicted_Label': [label_encoder.classes_[i] for i in y_test_pred],\n",
    "    'Confidence': confidence_scores,\n",
    "    'Correct': correct_mask\n",
    "})\n",
    "\n",
    "predictions_df.to_csv('../data/results/svm_predictions.csv', index=False)\n",
    "print(\"‚úÖ Predictions saved to: ../data/results/svm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "# 14. Update Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "update-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UPDATING MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load existing comparison if exists\n",
    "try:\n",
    "    comparison_df = pd.read_csv('../data/results/model_comparison_all.csv')\n",
    "    print(\"\\nüìã Existing models in comparison:\")\n",
    "    print(comparison_df['Metric'].tolist() if 'Metric' in comparison_df.columns else \"Found comparison file\")\n",
    "except:\n",
    "    comparison_df = None\n",
    "    print(\"\\n‚ö†Ô∏è No existing comparison file found. Creating new one.\")\n",
    "\n",
    "# Create new comparison including SVM\n",
    "new_comparison = {\n",
    "    'Metric': ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score', \n",
    "               'Training Time (sec)', 'Prediction Time (ms)', 'Overfitting Gap'],\n",
    "    'SVM': [f'{test_accuracy*100:.2f}%', f'{test_precision*100:.2f}%', \n",
    "            f'{test_recall*100:.2f}%', f'{test_f1*100:.2f}%',\n",
    "            f'{grid_search_time:.4f}', f'{test_pred_time/len(y_test)*1000:.4f}',\n",
    "            f'{accuracy_diff*100:.2f}%']\n",
    "}\n",
    "\n",
    "if comparison_df is not None and 'Logistic_Regression' in comparison_df.columns:\n",
    "    comparison_df['SVM'] = new_comparison['SVM']\n",
    "else:\n",
    "    comparison_df = pd.DataFrame(new_comparison)\n",
    "\n",
    "comparison_df.to_csv('../data/results/model_comparison_all.csv', index=False)\n",
    "print(\"\\n‚úÖ Model comparison updated: ../data/results/model_comparison_all.csv\")\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "# 15. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK 09 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üìä SVM MODEL TRAINING SUMMARY:\n",
    "\n",
    "‚úÖ Trained SVM models with 3 different kernels (Linear, RBF, Polynomial)\n",
    "‚úÖ Performed hyperparameter tuning using GridSearchCV with 5-fold CV\n",
    "‚úÖ Achieved {:.2f}% test accuracy with best model\n",
    "‚úÖ Generated comprehensive evaluation metrics and visualizations\n",
    "‚úÖ Analyzed support vectors distribution across classes\n",
    "‚úÖ Saved trained model and results\n",
    "\n",
    "üìÅ OUTPUT FILES:\n",
    "   ‚Ä¢ Model: ../models/svm_model.pkl\n",
    "   ‚Ä¢ Classification Report: ../data/results/svm_classification_report.csv\n",
    "   ‚Ä¢ Summary: ../data/results/svm_summary.csv\n",
    "   ‚Ä¢ Predictions: ../data/results/svm_predictions.csv\n",
    "   ‚Ä¢ Visualizations: ../data/visualizations/40-43_svm_*.png\n",
    "\n",
    "üîÑ NEXT STEPS:\n",
    "   ‚Ä¢ Notebook 10: XGBoost and LightGBM Training\n",
    "   ‚Ä¢ Notebook 11: Stacking Ensemble Model\n",
    "   ‚Ä¢ Notebook 12: Final Model Comparison and Selection\n",
    "\"\"\".format(test_accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
